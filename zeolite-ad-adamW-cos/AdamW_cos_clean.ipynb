{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdamW-cos-clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxAJz4dDiDvYjmJCiI06hF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaoyan-Fu/amptorch/blob/active_learning/zeolite-ad-adamW-cos/AdamW_cos_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNyGidvNDWB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q git+https://github.com/Xiaoyan-Fu/amptorch.git@active_learning\n",
        "!pip install -q git+https://github.com/mshuaibii/SIMPLE-NN.git\n",
        "!pip install -q amp-atomistics\n",
        "!pip install -q --upgrade dftbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COyAAheCFYvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ase.io\n",
        "from amp import Amp\n",
        "from amp.model.neuralnetwork import NeuralNetwork\n",
        "from amp.model import LossFunction\n",
        "import operator\n",
        "import amptorch\n",
        "import copy\n",
        "import matplotlib\n",
        "from skorch import NeuralNetRegressor\n",
        "from skorch.dataset import CVSplit\n",
        "from skorch.callbacks import Checkpoint, EpochScoring\n",
        "from skorch.callbacks.lr_scheduler import LRScheduler\n",
        "import skorch.callbacks.base\n",
        "from amptorch.gaussian import SNN_Gaussian\n",
        "from amptorch.model import BPNN, CustomMSELoss\n",
        "from amptorch.skorch_model import AMP\n",
        "from amptorch.skorch_model.utils import target_extractor\n",
        "from amptorch.analysis import parity_plot\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import init\n",
        "from skorch.utils import to_numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from amptorch.modifications import AtomsDataset_per_image, CustomMSELoss_per_image, energy_score, AMPCalculator\n",
        "from amptorch.data_preprocess import collate_amp\n",
        "!wget https://github.com/Xiaoyan-Fu/amptorch/raw/active_learning/zeolite-ad-adamW-cos/traj_taged_adsorptionenergy.traj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfCqr487GOUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = ase.io.read('./traj_taged_adsorptionenergy.traj', index=':')\n",
        "def Split(images):\n",
        "  '''random split'''\n",
        "  args = (np.arange(len(images)),)\n",
        "  cv = ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n",
        "  idx_train, idx_test = next(iter(cv.split(*args, groups=None)))\n",
        "  train_images = [images[index] for index in idx_train]\n",
        "  test_images = [images[index] for index in idx_test]\n",
        "  return train_images, test_images\n",
        "train_images_original, test_images_original = Split(images)\n",
        "train_images = copy.deepcopy(train_images_original)\n",
        "test_images = copy.deepcopy(test_images_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLciA5BrGl-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assignments = {\n",
        "    'epochs': 3000,\n",
        "    'learning_rate': 0.02,\n",
        "    'hidden_layers': 4,\n",
        "    'num_nodes': 70,\n",
        "    'optimizer' : optim.AdamW,\n",
        "    'batchsize' : 40,\n",
        "    'T_max' : 3000,\n",
        "    'weight_decay': 0.1,\n",
        "    'LRScheduler': CosineAnnealingLR\n",
        "}\n",
        "Gs = {}\n",
        "Gs[\"G2_etas\"] = np.logspace(np.log10(0.05), np.log10(5.0), num=4)\n",
        "Gs[\"G2_rs_s\"] = [0] * 4\n",
        "Gs[\"G4_etas\"] = [0.005]\n",
        "Gs[\"G4_zetas\"] = [1.0]\n",
        "Gs[\"G4_gammas\"] = [+1.0, -1]\n",
        "Gs[\"cutoff\"] = 6.5\n",
        "# Gs define\n",
        "DFT_energies_test = [image.get_potential_energy() for image in test_images]\n",
        "torch.set_num_threads(1)\n",
        "# loads best validation loss at the end of training\n",
        "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
        "    def on_train_end(self, net, X, y):\n",
        "        net.load_params('valid_best_params.pt')\n",
        "cp = Checkpoint(monitor='valid_loss_best', fn_prefix='valid_best_')\n",
        "load_best_valid_loss = train_end_load_best_valid_loss()\n",
        "# hyperparameters and Gs defination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoAIZIeoUauh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = AtomsDataset_per_image(train_images, SNN_Gaussian, Gs, forcetraining=False,\n",
        "        label=\"zeolite\", cores=1, delta_data=None, specific_atoms=True)\n",
        "# database defination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og2agfP7TXoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = NeuralNetRegressor(\n",
        "    module=BPNN(\n",
        "        training_data.elements,\n",
        "        [training_data.fp_length, assignments[\"hidden_layers\"], assignments[\"num_nodes\"]],\n",
        "        \"cpu\",\n",
        "        forcetraining=False,\n",
        "    ),\n",
        "    criterion=CustomMSELoss_per_image,\n",
        "    criterion__force_coefficient=0,\n",
        "    optimizer=assignments[\"optimizer\"],\n",
        "    lr=assignments[\"learning_rate\"],\n",
        "    batch_size=assignments[\"batchsize\"],\n",
        "    max_epochs=assignments[\"epochs\"],\n",
        "    iterator_train__collate_fn=collate_amp,\n",
        "    iterator_train__shuffle=False,\n",
        "    iterator_valid__collate_fn=collate_amp,\n",
        "    iterator_valid__shuffle=False,\n",
        "    optimizer__weight_decay=assignments[\"weight_decay\"],\n",
        "    device=\"cpu\",\n",
        "    train_split=CVSplit(5),\n",
        "    callbacks=[\n",
        "        EpochScoring(\n",
        "            energy_score,\n",
        "            on_train=True,\n",
        "            use_caching=True,\n",
        "            target_extractor=target_extractor,\n",
        "        ),\n",
        "        ('lr_scheduler',\n",
        "          LRScheduler(policy=assignments['LRScheduler'], T_max=assignments[\"T_max\"],)\n",
        "        ),\n",
        "        cp,\n",
        "        load_best_valid_loss,\n",
        "        # LR_schedule\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1-SfyZ9VK2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "calc = AMP(training_data, net, 'zeolite', specific_atoms=True)\n",
        "calc.train(overwrite=True)\n",
        "energy_rmse_train = net.history[:, ('train_loss')]\n",
        "energy_rmse_valid = net.history[:, ('valid_loss')]\n",
        "for image in test_images:\n",
        "  image.set_calculator(calc)\n",
        "pred_energies_test = [image.get_potential_energy() for image in test_images]\n",
        "energy_rmse_test = np.sqrt(mean_squared_error(pred_energies_test, DFT_energies_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z79vTa_9VmxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "experiment = 'adamW_minibatch_cos'\n",
        "epoch = assignments['epochs']\n",
        "epochs = [i for i in range(1,epoch+1)]\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        'RMSE': energy_rmse_train + energy_rmse_valid,\n",
        "        'Experiment':[experiment] * (epoch * 2),\n",
        "        'Category': ['Train'] * epoch + ['valid']  * epoch,\n",
        "        'epochs': epochs * 2,\n",
        "    })\n",
        "g = sns.relplot(x=\"epochs\", y=\"RMSE\", hue=\"Category\", kind=\"line\", data=df)\n",
        "stdv = np.std([image.get_potential_energy() for image in images])\n",
        "g.ax.text(max(df['epochs']), stdv, 'std')\n",
        "g.ax.plot([0, max(df['epochs'])], [stdv, stdv], ls=':',linewidth=1)\n",
        "g.ax.set_ylim(0,5)\n",
        "label = 'test_RMSE'\n",
        "value = energy_rmse_test\n",
        "g.ax.text(max(df['epochs']), value, label)\n",
        "g.ax.plot([0, max(df['epochs'])], [value, value], ls=':',linewidth=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}